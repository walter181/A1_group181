{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCbAmQ47iqK4"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# FIT5196 Task 1 in Assessment 1\n",
    "#### Student Name: Deshui Yu      Liangjing Yang\n",
    "#### Student ID: 34253599      34060871\n",
    "\n",
    "Date: 21/08/2024\n",
    "\n",
    "\n",
    "Environment: Python xxxx\n",
    "\n",
    "Libraries used:\n",
    "* re (for regular expression, installed and imported) \n",
    "* pandas (for data manipulation) \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjBFqYK4iqK5"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "## Table of Contents\n",
    "\n",
    "</div>    \n",
    "\n",
    "[1. Introduction](#Intro) <br>\n",
    "[2. Importing Libraries](#libs) <br>\n",
    "[3. Examining Patent Files](#examine) <br>\n",
    "[4. Loading and Parsing Files](#load) <br>\n",
    "$\\;\\;\\;\\;$[4.1. Defining Regular Expressions](#Reg_Exp) <br>\n",
    "$\\;\\;\\;\\;$[4.2. Reading Files](#Read) <br>\n",
    "$\\;\\;\\;\\;$[4.3. Whatever else](#latin) <br>\n",
    "[5. Writing to CSV/JSON File](#write) <br>\n",
    "$\\;\\;\\;\\;$[5.1. Verification - using the sample files](#test_xml) <br>\n",
    "[6. Summary](#summary) <br>\n",
    "[7. References](#Ref) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcbqK3KliqK6"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEFdSCIUiqK6"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGg4-8VSiqK6"
   },
   "source": [
    "This assessment regards extracting data from semi-sctuctured text files. The dataset contained 15 `.txt` files and 1 `.xlsx` which included various information about user reviews. In particular, Gmap_id(Google Maps identifier for the review location.),Pictures(Links to related pictures (if available).),UserId(Identifier of the user who posted the review.),Date(Timestamp),rate(Numerical rating),review(the user review),name(review poster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6Ql-W6BiqK7"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnctlBF6iqK7"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQOLlwmAiqK7"
   },
   "source": [
    "The packages to be used in this assessment are imported in the following. They are used to fulfill the following tasks:\n",
    "\n",
    "* **re:** to define and use regular expressions\n",
    "* **pandas:** to manage and analyze data.\n",
    "* **datetime** to handle dates and times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mKGO6FAXiqK7"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# My assignment is running locally, and I have already downloaded the data to my local system, so I don't need to link to Google Drive.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DEWD9qIiqK8"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Z814ttFYiqK8"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## 3.  Examining Raw Data <a class=\"anchor\" name=\"examine\"></a>\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YVIqb_miqK8"
   },
   "source": [
    "First of all, open all the files and examine the data within them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gp3TG3fyiqK9"
   },
   "source": [
    "Having examined the file content, the following observations were made:\n",
    "\n",
    "Examining the XLSX File:\n",
    "- By opening the XLSX file, I discovered that there are sheets numbered from 0 to 14. This means I need to combine these sheets into a single DataFrame.\n",
    "- By observing the different sheets, I noticed that some of them contain entire rows and columns of empty values. I think I need to remove these empty values.\n",
    "- After removing the irrelevant empty rows and columns, I found that all sheets have the same content. However, the format of the date and other data does not meet the requirements of the assignment.\n",
    "\n",
    "Examining the TXT Files:\n",
    "- I have 15 files numbered from 0 to 14. The first line of each TXT file contains the label `<?xml version=\"1.0\" encoding=\"UTF-8\"?>`, which is a common point. I can use this to split the data.\n",
    "- I also noticed that the label `</dataset><record>` can be used to split the data for each user, and this pattern is consistent across all TXT files.\n",
    "- In the TXT files, each piece of data is wrapped in tags, such as userid, username, rate, pictures, etc. Additionally, the tags are quite varied; for instance, the rate data can have multiple tag forms such as `<Rate>`, `</rate>`, `<rating>`, `</Rating>`, and more. This means that if I want to extract the data, my regular expressions will need to be compatible with these different tag forms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBEASWLfiqK-"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "iDoVeDSHiqK-"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 4.  Loading and Parsing Files <a class=\"anchor\" name=\"load\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z24HaN8hiqK-"
   },
   "source": [
    "In this section, the files are parsed and processed. First of all, appropriate regular expressions are defined to extract desired information when reading the files. After reading the file information, the data is segmented because it appears concatenated when extracted from the .txt files. Regular expressions are then used to filter the same types of data, which are subsequently stored in a dataframe. Next, the .xlsx file is read, and the data is processed according to the assignment's requirements. Finally, the data is concatenated, and files are exported and generated according to the specifications of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ0tuwvZiqK-"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rApp_Ic9iqK-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.1. Defining Regular Expressions <a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knauV0VViqK-"
   },
   "source": [
    "Defining correct regular expressions is crucial in extracting desired information from the text efficiently.  It helps in filtering out and retrieving specific types of data from the raw data read from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_expression = r\"<[^>]*?ser[^>]*?>\\s*(\\d{21})\\s*<[^>]*?/[^>]*?ser[^>]*?>\"\n",
    "time_expression = r\"(?:<.{1,3}ime\\s*/?>\\s*|<.{0,2}[Dd]ate\\s*/?>\\s*)(\\d+)(?:\\s*<\\s*/\\s*.{1,3}ime\\s*/?>|<\\s*/\\s*.{0,2}[Dd]ate\\s*/?>)\"\n",
    "rate_expression = r\"(?:<.{0,2}[Rr]ate>\\s*)(\\d+)(?:<.{0,2}[Rr]ate>)\"\n",
    "rateing_expression = r\"(?:<.{0,2}[Rr]ating>\\s*)(\\d+)(?:<.{0,2}[Rr]ating>)\"\n",
    "text_expression = r\"(?:<.{0,3}[Tt]ext>\\s*)(.*?)(?:<.{0,3}[Tt]ext>)\"\n",
    "review_expression = r\"(?:<.{0,2}[Rr]eview>\\s*)(.*?)(?:<.{0,2}[Rr]eview>)\"\n",
    "name_expression = r\"(?:<.{0,7}[Nn]ame>\\s*)(.*?)(?:<.{0,7}[Nn]ame\\s*>)\"\n",
    "response_expression = r\"(?:<.{0,3}[Rr]esp.{0,4}>\\s*)(.*?)(?:<.{0,3}[Rr]esp.{0,4}>)\"\n",
    "picture_expression = r\"(?:<.{0,2}[Pp]ic.{1,5}>\\s*)(.*?)(?:<.{0,2}[Pp]ic.{1,5}>)\"\n",
    "gmap_id_expression=r\"(?:<.{0,2}[Gg]map.{1,3}>\\s*)(.*?)(?:<.{0,2}[Gg]map.{1,3}>)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTqmBHIKiqK_"
   },
   "source": [
    "These patterns are used in the next step when reading the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZ-njkJciqK_"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGcAMvmhiqK_"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.2. Reading Files <a class=\"anchor\" name=\"Read\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llC5D5M3iqK_"
   },
   "source": [
    "In this step, all files are read and parsed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Reading txt file\n",
    "My code and data are stored locally, so I set a path to the text files (./student_group181) and initialized an empty string all_text_data to store the data. Using a list called file_names, I listed 15 text files, ranging from 'group181_0.txt' to 'group181_14.txt'. The code then iterates through each file in a loop, opens them, reads their contents, and appends this content to the all_text_data string. This approach merges the text contents of multiple files into a single string, facilitating subsequent data processing or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The path stored the text files\n",
    "text_path = './student_group181'\n",
    "#Initializes an empty string to store text data\n",
    "all_text_data = \"\"\n",
    "file_names = [\n",
    "    'group181_0.txt', 'group181_1.txt', 'group181_2.txt', 'group181_3.txt', \n",
    "    'group181_4.txt', 'group181_5.txt', 'group181_6.txt', 'group181_7.txt', \n",
    "    'group181_8.txt', 'group181_9.txt', 'group181_10.txt', 'group181_11.txt',\n",
    "    'group181_12.txt', 'group181_13.txt', 'group181_14.txt'\n",
    "]\n",
    "\n",
    "#Read the contents of multiple text files and merge them together\n",
    "for file_name in file_names:\n",
    "    file_path = f\"{text_path}/{file_name}\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        all_text_data += content\n",
    "        \n",
    "#The path stored the text files\n",
    "text_path = './student_group181'\n",
    "#Initializes an empty string to store text data\n",
    "all_text_data = \"\"\n",
    "file_names = [\n",
    "    'group181_0.txt', 'group181_1.txt', 'group181_2.txt', 'group181_3.txt', \n",
    "    'group181_4.txt', 'group181_5.txt', 'group181_6.txt', 'group181_7.txt', \n",
    "    'group181_8.txt', 'group181_9.txt', 'group181_10.txt', 'group181_11.txt',\n",
    "    'group181_12.txt', 'group181_13.txt', 'group181_14.txt'\n",
    "]\n",
    "\n",
    "#Read the contents of multiple text files and merge them together\n",
    "for file_name in file_names:\n",
    "    file_path = f\"{text_path}/{file_name}\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        all_text_data += content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Split Data  and Data Extraction\n",
    "I first utilized specific XML tags as delimiters to split the string containing all text data (all_text_data) into a list (folder_text). This step is effective due to the consistency of XML tags and the structured nature of the data. Any empty strings encountered at the beginning of the split list are removed to ensure data cleanliness and accuracy in subsequent processing.\n",
    "Next, I applied regular expressions to further segment each section of text, ensuring accurate classification and separation. These segments are stored in the list final_split_text, which is used later for detailed data extraction.\n",
    "In the extraction phase, I initialized multiple empty lists to store various types of data, including user IDs, Google Maps IDs, timestamps, ratings, review contents, responses, image links, and usernames. This organization facilitates structured storage of the extracted data.\n",
    "Using precompiled regular expression patterns, I traversed all the text in final_split_text, extracting the required information through precise pattern matching. Each successful match resulted in the corresponding data being appended to its respective list. This approach ensures efficiency and accuracy in data extraction, preparing it for further analysis and application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt = '?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n",
    "final_split= '</dataset><record>|<dataset><record>'\n",
    "\n",
    "#All text data is divided by XML to get a list  \n",
    "folder_text = all_text_data.split(split_txt)\n",
    "\n",
    "#Delete an empty string\n",
    "if folder_text[0] == \"\":\n",
    "    folder_text.pop(0)\n",
    "\n",
    "#Split the text again according to the regular expression\n",
    "final_split_text = []\n",
    "for text in folder_text:\n",
    "    slipted_info = re.split(final_split,text)\n",
    "    final_split_text.extend(slipted_info)\n",
    "    \n",
    "#Creates an empty list for storing text\n",
    "user_id_list = []\n",
    "gmap_id_list = []\n",
    "time_list = []\n",
    "date_list = []\n",
    "rate_list = []\n",
    "review_list = []\n",
    "response_list = []\n",
    "picture_list = []\n",
    "name_list = []\n",
    "\n",
    "#Use regular expression compilations to match specific text structures\n",
    "user_id_pattern = re.compile(user_id_expression, re.DOTALL)\n",
    "time_pattern = re.compile(time_expression, re.DOTALL)\n",
    "rate_pattern = re.compile(rate_expression, re.DOTALL)\n",
    "rateing_pattern = re.compile(rateing_expression, re.DOTALL)\n",
    "text_pattern = re.compile(text_expression, re.DOTALL)\n",
    "review_pattern = re.compile(review_expression, re.DOTALL)\n",
    "response_pattern = re.compile(response_expression, re.DOTALL)\n",
    "picture_pattern = re.compile(picture_expression, re.DOTALL)\n",
    "name_pattern = re.compile(name_expression,re.DOTALL)\n",
    "gmap_id_pattern = re.compile(gmap_id_expression, re.DOTALL)\n",
    "\n",
    "#Walk through all the segmented text, matching and extracting the corresponding text\n",
    "for text in final_split_text:\n",
    "     \n",
    "     user_id_matches = user_id_pattern.findall(text)\n",
    "     user_id_list.extend(user_id_matches)\n",
    "     \n",
    "     gmap_id_matches = gmap_id_pattern.findall(text)\n",
    "     gmap_id_list.extend(gmap_id_matches)\n",
    "     \n",
    "     time_matches = time_pattern.findall(text)\n",
    "     time_list.extend(time_matches)\n",
    "     \n",
    "     rate_matches = rate_pattern.findall(text)\n",
    "     if not rate_matches:\n",
    "        rate_matches = rateing_pattern.findall(text)\n",
    "     rate_list.extend(rate_matches)\n",
    "     \n",
    "     review_matches = review_pattern.findall(text)\n",
    "     if not review_matches:\n",
    "        review_matches = text_pattern.findall(text)\n",
    "     review_list.extend(review_matches)\n",
    "     \n",
    "     response_matches = response_pattern.findall(text)\n",
    "     response_list.extend(response_matches)\n",
    "     \n",
    "     picture_matches = picture_pattern.findall(text)\n",
    "     picture_list.extend(picture_matches)\n",
    "     \n",
    "     \n",
    "     name_matches = name_pattern.findall(text)\n",
    "     name_list.extend(name_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 DataFrame data processing\n",
    "In this step, a DataFrame named text_datafram was created to store the information from the previous step. Then, according to the requirements for exporting file data for the assignment, the data type of the 'rating' column was converted to a floating point number, and the timestamp data was converted into a readable date-time format. Special strings labeled “None” were replaced with actual None values. The code iterates through each row of the DataFrame, checking the contents of the 'pics' and 'resp' columns to determine the presence of pictures and responses, and the results are added to new columns if_pic and if_response, respectively. Additionally, the function extract_dimensions extracts the width and height from the picture URLs and stores these dimensions in a new column, pic_dim, to meet the assignment's specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe from the extracted data\n",
    "text_datafram = pd.DataFrame({\n",
    "    'user_id': user_id_list,\n",
    "    'name':name_list,\n",
    "    'time': time_list,\n",
    "    'rating': rate_list,\n",
    "    'text': review_list,\n",
    "    'pics': picture_list,\n",
    "    'resp': response_list,\n",
    "    'gmap_id': gmap_id_list\n",
    "})\n",
    "\n",
    "#Arranges the column order of the dataframe\n",
    "text_datafram = text_datafram[['user_id', 'name', 'time', 'rating', 'text', 'pics', 'resp','gmap_id']]\n",
    "\n",
    "#Converts the data type of the rating column to float\n",
    "text_datafram['rating'] = text_datafram['rating'].astype(float)\n",
    "\n",
    "#Converts the timestamp of the time column to a date-time format and to a string\n",
    "text_datafram['time'] = pd.to_datetime(pd.to_numeric(text_datafram['time']), unit='ms', utc=True)\n",
    "text_datafram['time'] = text_datafram['time'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#Replace the string 'None' in the data box with the actual None value\n",
    "text_datafram = text_datafram.replace('None', None)\n",
    "\n",
    "#Iterate through each row of the data box and determine whether there are pictures and replies based on the contents of the 'pics' and 'resp' columns\n",
    "if_pic = []\n",
    "if_response = []\n",
    "for index, row in text_datafram.iterrows():\n",
    "    if row['pics'] is None:\n",
    "        if_pic.append('N')\n",
    "    else:\n",
    "        if_pic.append('Y')\n",
    "    \n",
    "    if row['resp'] is None:\n",
    "        if_response.append('N')\n",
    "    else:\n",
    "        if_response.append('Y')\n",
    "text_datafram['if_pic'] = if_pic\n",
    "text_datafram['if_response'] = if_response\n",
    "\n",
    "#Defines a function to extract the width and height of the picture from the picture URL and store it in the 'pic_dim' column\n",
    "def extract_dimensions(text):\n",
    "    if pd.isna(text):\n",
    "        return \"[ ]\"\n",
    "    dimensions = re.findall(r'w(\\d{1,3})-h(\\d{1,3})-k-no-p', text)\n",
    "    if not dimensions:\n",
    "        return \"[ ]\"\n",
    "    return [[int(width), int(height)] for width, height in dimensions]\n",
    "\n",
    "text_datafram['pic_dim'] = text_datafram['pics'].apply(extract_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5 Read xlsx and data processing\n",
    "Since the operation is conducted locally, I first specify the path to the Excel file and use the pandas library to read the file. I load the data from each worksheet, removing rows and columns that are entirely empty. Non-empty dataframes are added to the cleaned_sheets list. These cleaned dataframes are then merged into a single dataframe, xls_data. Timestamps are converted to a readable date-time format and formatted as strings according to the assignment requirements. In the 'pics' and 'resp' columns, all missing values are replaced with Python's None. I iterate through each row, determining the presence of pictures and responses based on the contents of the 'pics' and 'resp' columns, and record the results in newly added columns if_pic and if_response. I define a function extract_dimensions to extract the width and height from picture URLs and store them in the new column pic_dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the path to the Excel file and read the Excel file\n",
    "xls_path = \"./student_group181/group181.xlsx\"\n",
    "xls = pd.ExcelFile(xls_path)\n",
    "\n",
    "#Read data from all worksheets in an Excel file\n",
    "all_sheets = pd.read_excel(xls_path, sheet_name=None)\n",
    "\n",
    "#Iterate through each worksheet, removing all empty rows and columns and adding them to the list\n",
    "cleaned_sheets = []\n",
    "for sheet_name, df in all_sheets.items():\n",
    "    df = df.dropna(how='all')\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    if not df.empty:\n",
    "        cleaned_sheets.append(df)\n",
    "\n",
    "# Merge all cleaned dataframes into one\n",
    "xls_data = pd.concat(cleaned_sheets, ignore_index=True)\n",
    "xls_data = xls_data[['user_id', 'name', 'time', 'rating', 'text', 'pics', 'resp','gmap_id']]\n",
    "\n",
    "#Conversion time format\n",
    "xls_data['time'] = pd.to_datetime(xls_data['time'], unit ='ms', utc = True)\n",
    "\n",
    "#Format the time column into string format\n",
    "xls_data['time'] = xls_data['time'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#Replace the missing values in the 'pics' and 'resp' columns with None\n",
    "xls_data['pics'] = xls_data['pics'].where(pd.notna(xls_data['pics']), None)\n",
    "xls_data['resp'] = xls_data['resp'].where(pd.notna(xls_data['resp']), None)\n",
    "\n",
    "#Iterate through each row of the data box and determine whether there are pictures and replies based on the contents of the 'pics' and 'resp' columns\n",
    "if_pic_xls = []\n",
    "if_response_xls = []\n",
    "for index, row in xls_data.iterrows():\n",
    "    if row['pics'] is None:\n",
    "        if_pic_xls.append('N')\n",
    "    else:\n",
    "        if_pic_xls.append('Y')\n",
    "    \n",
    "    if row['resp'] is None:\n",
    "        if_response_xls.append('N')\n",
    "    else:\n",
    "        if_response_xls.append('Y')\n",
    "\n",
    "xls_data['if_pic'] = if_pic_xls\n",
    "xls_data['if_response'] = if_response_xls\n",
    "\n",
    "#Defines a function to extract the width and height of the picture from the picture URL and store it in the 'pic_dim' column\n",
    "def extract_dimensions(text):\n",
    "    if pd.isna(text):\n",
    "        return \"[ ]\"\n",
    "    dimensions = re.findall(r'w(\\d{1,3})-h(\\d{1,3})-k-no-p', text)\n",
    "    if not dimensions:\n",
    "        return \"[ ]\"\n",
    "    return [[int(width), int(height)] for width, height in dimensions]\n",
    "\n",
    "xls_data['pic_dim'] = xls_data['pics'].apply(extract_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-brytUfiqLA"
   },
   "source": [
    "To view the data in text_datafram and xls_data and ensure accuracy for subsequent data merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_datafram.info()\n",
    "xls_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 user_id             name                 time  rating  \\\n",
      "0  103499789493991350231   Alyson Hoffman  2021-06-01 04:57:49     5.0   \n",
      "1  105404622065259965715     OnlyOneEarth  2021-03-21 07:59:56     5.0   \n",
      "2  118347770680889723845  matthew thomson  2021-04-11 15:44:41     5.0   \n",
      "3  109541606733835908756          Becky B  2021-03-15 15:42:06     5.0   \n",
      "4  108792737827746934756  Tiffany Klitzke  2021-02-02 18:00:03     5.0   \n",
      "\n",
      "                                                text  \\\n",
      "0  Booked last minute rentals here over Memorial ...   \n",
      "1  When I went into Epicenter Cycling, I was look...   \n",
      "2  Amazing selection of Trek bikes! We were told ...   \n",
      "3  Absolutely flawless experience ordering, sizin...   \n",
      "4  All of the employees were so friendly and help...   \n",
      "\n",
      "                                                pics  \\\n",
      "0                                               None   \n",
      "1  [{'url': ['https://lh5.googleusercontent.com/p...   \n",
      "2                                               None   \n",
      "3  [{'url': ['https://lh5.googleusercontent.com/p...   \n",
      "4                                               None   \n",
      "\n",
      "                                                resp  \\\n",
      "0                                               None   \n",
      "1  {'time': 1616349255531, 'text': \"We're so happ...   \n",
      "2  {'time': 1622404153725, 'text': \"Thanks for ta...   \n",
      "3  {'time': 1616264680468, 'text': 'Thank you so ...   \n",
      "4  {'time': 1614824613439, 'text': \"Thank you so ...   \n",
      "\n",
      "                                 gmap_id if_pic if_response  \\\n",
      "0  0x808e6a7f6b666d4f:0xfd9214abe1df3a4d      N           N   \n",
      "1  0x808e6a7f6b666d4f:0xfd9214abe1df3a4d      Y           Y   \n",
      "2  0x808e6a7f6b666d4f:0xfd9214abe1df3a4d      N           Y   \n",
      "3  0x808e6a7f6b666d4f:0xfd9214abe1df3a4d      Y           Y   \n",
      "4  0x808e6a7f6b666d4f:0xfd9214abe1df3a4d      N           Y   \n",
      "\n",
      "                    pic_dim  \n",
      "0                       [ ]  \n",
      "1  [[150, 150], [150, 150]]  \n",
      "2                       [ ]  \n",
      "3              [[150, 150]]  \n",
      "4                       [ ]  \n",
      "                 user_id                           name                 time  \\\n",
      "0  117547450879418399308             Elizabeth Santiago  2021-01-27 14:22:58   \n",
      "1  105343328635393398077                  michael britt  2021-02-12 19:17:37   \n",
      "2  110224938425929427087  Jerome “Ray Cyrus” Vonloodwig  2021-01-14 19:56:47   \n",
      "3  111601567646818950873                   Karla Huling  2020-10-07 17:20:37   \n",
      "4  100829865257284707673                   Moore Family  2021-02-26 21:10:26   \n",
      "\n",
      "   rating                                               text  pics  resp  \\\n",
      "0     2.0  Extremely inhumane...that's how these people m...  None  None   \n",
      "1     1.0  This guy with tattos on both arms was extremel...  None  None   \n",
      "2     1.0  If you are forced to come here for worksman co...  None  None   \n",
      "3     1.0  HORRIBLE CLINIC. DO NOOOT GO HERE! I REPEAT DO...  None  None   \n",
      "4     1.0  If i could leave a 0 stars for this place I wo...  None  None   \n",
      "\n",
      "                                 gmap_id if_pic if_response pic_dim  \n",
      "0  0x80ea6992ca7cd91f:0xb0a098c869f2e8eb      N           N     [ ]  \n",
      "1  0x80ea6992ca7cd91f:0xb0a098c869f2e8eb      N           N     [ ]  \n",
      "2  0x80ea6992ca7cd91f:0xb0a098c869f2e8eb      N           N     [ ]  \n",
      "3  0x80ea6992ca7cd91f:0xb0a098c869f2e8eb      N           N     [ ]  \n",
      "4  0x80ea6992ca7cd91f:0xb0a098c869f2e8eb      N           N     [ ]  \n"
     ]
    }
   ],
   "source": [
    "print(text_datafram.head())\n",
    "print(xls_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MD-LSS76iqLA"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtwS6ttqiqLA"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.3. Whatever else <a class=\"anchor\" name=\"latin\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I merged data extracted from Excel and text files into a new dataframe named combined_data, using the pd.concat method to append xls_data and text_datafram vertically (axis=0). Subsequently, I employed a regular expression to remove special characters and emoticons from the text column. This was achieved by applying a function that checks for string instances and then strips out a specified range of Unicode emoticon characters. All text was then converted to lowercase to standardize the data format for easier processing. Finally, the combined dataframe was exported to a CSV file combined_data.csv without saving the row indices. This streamlined process ensures data consistency and prepares it for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge data from excel files and txt files\n",
    "combined_data = pd.concat([xls_data, text_datafram], axis=0)\n",
    "\n",
    "#Remove special characters and emoticons from text with regular expressions\n",
    "combined_data['text'] = combined_data['text'].apply(\n",
    "    lambda x: re.sub(\n",
    "        r\"[\\U0001F600-\\U0001F64F\"  \n",
    "        r\"\\U0001F300-\\U0001F5FF\"  \n",
    "        r\"\\U0001F680-\\U0001F6FF\"  \n",
    "        r\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        r\"\\U00002702-\\U000027B0\"  \n",
    "        r\"\\U000024C2-\\U0001F251\"  \n",
    "        r\"]+\", '', x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "#Convert all text to lowercase\n",
    "combined_data['text'] = combined_data['text'].str.lower()\n",
    "combined_data.to_csv('combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g9k9Fb8iqLB"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "KVwmp1LfiqLE"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 5.  Writing to JSON File <a class=\"anchor\" name=\"write\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create csv File\n",
    "The combined_data dataframe is grouped by the gmap_id column, and aggregated to calculate the total number of reviews, the count of reviews containing text, and the count of reviews with responses for each group. The agg method is used for these calculations, where review_count measures the total reviews per group, review_text_count counts non-empty text fields, and response_count tallies non-empty response fields. The results are reset using reset_index for easier data access in subsequent steps. Finally, the processed data is saved to a CSV file named task1_181.csv, with index=False to prevent the index from being written into the file, ensuring a cleaner file content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the data boxes by the 'gmap_id' column and count the amount of corresponding text\n",
    "grouped = combined_data.groupby('gmap_id').agg(\n",
    "    review_count=('gmap_id', 'size'),\n",
    "    review_text_count=('text', lambda x: x.notna().sum()),\n",
    "    response_count=('resp', lambda x: x.notna().sum())\n",
    ").reset_index()\n",
    "\n",
    "#Store data as CSV file\n",
    "grouped.to_csv('task1_181.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create json File\n",
    "I created a new DataFrame, data_selected, and grouped the data by gmap_id to iterate through it. If picture information is present, the pic_dim column data is converted from list format to string format. For each group, I constructed a list of reviews that includes each review's user ID, time, rating, text, presence of a picture, picture dimensions, and whether there was a response (as required by the assignment). Each gmap_id and its corresponding list of reviews, along with the earliest and latest review dates for that group, are stored in the dictionary output_data. Finally, I saved the output_data dictionary in JSON format to a file named task1_181.json, using UTF-8 encoding and formatted output to enhance readability. This process effectively transforms structured data into a JSON format for further data sharing and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#Select the specified column and create a new DataFrame\n",
    "data_selected = combined_data[['gmap_id', 'user_id', 'time', 'rating', 'text', 'if_pic', 'pic_dim', 'if_response','pics']]\n",
    "\n",
    "#Groups the data by 'gmap_id', traverses the rows of data within each group, and converts the data in pic_dim to a string if picture information is present\n",
    "output_data = {}\n",
    "for gmap_id, group in data_selected.groupby('gmap_id'):\n",
    "    reviews = []\n",
    "    earliest_review_date = group['time'].min()\n",
    "    latest_review_date = group['time'].max()\n",
    "    for _, row in group.iterrows():\n",
    "        pic_dim_converted = [[str(dim) for dim in dims] for dims in row['pic_dim']] if pd.notna(row['pics']) else []\n",
    "\n",
    "        #Build the structure of a single review\n",
    "        review = {\n",
    "            \"user_id\": row['user_id'],\n",
    "            \"time\": row['time'],\n",
    "            \"review_rating\": row['rating'],\n",
    "            \"review_text\": row['text'],\n",
    "            \"if_pic\": row['if_pic'],\n",
    "            'pic_dim': pic_dim_converted,\n",
    "            \"if_response\": row['if_response']\n",
    "        }\n",
    "        #Add review data to the reviews list\n",
    "        reviews.append(review)\n",
    "    #Store each gmap_id and its corresponding comment list in output_data\n",
    "        output_data[gmap_id] = {\n",
    "        \"reviews\": reviews,\n",
    "        \"earliest_review_date\": earliest_review_date,\n",
    "        \"latest_review_date\": latest_review_date\n",
    "    }\n",
    "\n",
    "# Store data as json file\n",
    "with open('task1_181.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, indent=4, ensure_ascii=False, separators=(',', ': '))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFHcHFPGiqLE"
   },
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0WT10TJiqLE"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "3XcaJBATiqLE"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 5.1. Verification of the Generated JSON File <a class=\"anchor\" name=\"test_xml\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the task1_test.py shared by the tutor, I verified my JSON data, and my data file passed the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO8vwKqkiqLF"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 6. Summary <a class=\"anchor\" name=\"summary\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEg_xQdXiqLF"
   },
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assessment, I processed data from multiple TXT and XLSX files, merging them into a unified DataFrame. The TXT files were parsed using regular expressions to extract relevant information, while the XLSX sheets were cleaned and combined. The data was then standardized, with special characters and emoticons removed, text converted to lowercase, and timestamps formatted. I grouped and aggregated the data by gmap_id, calculating review counts and response presence, and saved the results into CSV and JSON formats for further analysis. This process ensured clean, consistent data ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QdX7ozQiqLF"
   },
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnASfTmniqLF"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li7bchX9iqLF"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 7. References <a class=\"anchor\" name=\"Ref\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkWuWC3NiqLF"
   },
   "source": [
    "\n",
    "\n",
    "[1]<a class=\"anchor\" name=\"ref-2\"></a> Why do I need to add DOTALL to python regular expression to match new line in raw string, https://stackoverflow.com/questions/22610247, Accessed 30/08/2022.\n",
    "\n",
    "We use ChatGPT to help us improve text expression, correct grammar, provide assignment step suggestions, offer code advice, debug code, and check and enhance regular expressions.https://chatgpt.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVyuz4LciqLG"
   },
   "source": [
    "## --------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task1_xxxxxxx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
