{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCbAmQ47iqK4"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# FIT5196 Task 1 in Assessment 1\n",
    "#### Student Name: Deshui Yu      Liangjing Yang\n",
    "#### Student ID: 34253599      34060871\n",
    "\n",
    "Date: 21/08/2024\n",
    "\n",
    "\n",
    "Environment: Python xxxx\n",
    "\n",
    "Libraries used:\n",
    "* re (for regular expression, installed and imported) \n",
    "* pandas (for data manipulation) \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjBFqYK4iqK5"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "## Table of Contents\n",
    "\n",
    "</div>    \n",
    "\n",
    "[1. Introduction](#Intro) <br>\n",
    "[2. Importing Libraries](#libs) <br>\n",
    "[3. Examining Patent Files](#examine) <br>\n",
    "[4. Loading and Parsing Files](#load) <br>\n",
    "$\\;\\;\\;\\;$[4.1. Defining Regular Expressions](#Reg_Exp) <br>\n",
    "$\\;\\;\\;\\;$[4.2. Reading Files](#Read) <br>\n",
    "$\\;\\;\\;\\;$[4.3. Whatever else](#latin) <br>\n",
    "[5. Writing to CSV/JSON File](#write) <br>\n",
    "$\\;\\;\\;\\;$[5.1. Verification - using the sample files](#test_xml) <br>\n",
    "[6. Summary](#summary) <br>\n",
    "[7. References](#Ref) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcbqK3KliqK6"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEFdSCIUiqK6"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGg4-8VSiqK6"
   },
   "source": [
    "This assessment regards extracting data from semi-sctuctured text files. The dataset contained 500 `.txt` files which included various information about user reviews. In particular, ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6Ql-W6BiqK7"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnctlBF6iqK7"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQOLlwmAiqK7"
   },
   "source": [
    "The packages to be used in this assessment are imported in the following. They are used to fulfill the following tasks:\n",
    "\n",
    "* **re:** to define and use regular expressions\n",
    "* **pandas:** ...\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mKGO6FAXiqK7"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# My assignment is running locally, and I have already downloaded the data to my local system, so I don't need to link to Google Drive.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DEWD9qIiqK8"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Z814ttFYiqK8"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## 3.  Examining Raw Data <a class=\"anchor\" name=\"examine\"></a>\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YVIqb_miqK8"
   },
   "source": [
    "First of all, open all the files and examine the data within them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gp3TG3fyiqK9"
   },
   "source": [
    "Having examined the file content, the following observations were made:\n",
    "\n",
    "Examining the XLSX File:\n",
    "- By opening the XLSX file, I discovered that there are sheets numbered from 0 to 14. This means I need to combine these sheets into a single DataFrame.\n",
    "- By observing the different sheets, I noticed that some of them contain entire rows and columns of empty values. I think I need to remove these empty values.\n",
    "- After removing the irrelevant empty rows and columns, I found that all sheets have the same content. However, the format of the date and other data does not meet the requirements of the assignment.\n",
    "\n",
    "Examining the TXT Files:\n",
    "- I have 15 files numbered from 0 to 14. The first line of each TXT file contains the label `<?xml version=\"1.0\" encoding=\"UTF-8\"?>`, which is a common point. I can use this to split the data.\n",
    "- I also noticed that the label `</dataset><record>` can be used to split the data for each user, and this pattern is consistent across all TXT files.\n",
    "- In the TXT files, each piece of data is wrapped in tags, such as userid, username, rate, pictures, etc. Additionally, the tags are quite varied; for instance, the rate data can have multiple tag forms such as `<Rate>`, `</rate>`, `<rating>`, `</Rating>`, and more. This means that if I want to extract the data, my regular expressions will need to be compatible with these different tag forms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBEASWLfiqK-"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "iDoVeDSHiqK-"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 4.  Loading and Parsing Files <a class=\"anchor\" name=\"load\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z24HaN8hiqK-"
   },
   "source": [
    "In this section, the files are parsed and processed. First of all, appropriate regular expressions are defined to extract desired information when reading the files. ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ0tuwvZiqK-"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rApp_Ic9iqK-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.1. Defining Regular Expressions <a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knauV0VViqK-"
   },
   "source": [
    "Defining correct regular expressions is crucial in extracting desired information from the text efficiently. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_expression = r\"<[^>]*?ser[^>]*?>\\s*(\\d{21})\\s*<[^>]*?/[^>]*?ser[^>]*?>\"\n",
    "time_expression = r\"(?:<.{1,3}ime\\s*/?>\\s*|<.{0,2}[Dd]ate\\s*/?>\\s*)(\\d+)(?:\\s*<\\s*/\\s*.{1,3}ime\\s*/?>|<\\s*/\\s*.{0,2}[Dd]ate\\s*/?>)\"\n",
    "rate_expression = r\"(?:<.{0,2}[Rr]ate>\\s*)(\\d+)(?:<.{0,2}[Rr]ate>)\"\n",
    "rateing_expression = r\"(?:<.{0,2}[Rr]ating>\\s*)(\\d+)(?:<.{0,2}[Rr]ating>)\"\n",
    "text_expression = r\"(?:<.{0,3}[Tt]ext>\\s*)(.*?)(?:<.{0,3}[Tt]ext>)\"\n",
    "review_expression = r\"(?:<.{0,2}[Rr]eview>\\s*)(.*?)(?:<.{0,2}[Rr]eview>)\"\n",
    "name_expression = r\"(?:<.{0,7}[Nn]ame>\\s*)(.*?)(?:<.{0,7}[Nn]ame\\s*>)\"\n",
    "response_expression = r\"(?:<.{0,3}[Rr]esp.{0,4}>\\s*)(.*?)(?:<.{0,3}[Rr]esp.{0,4}>)\"\n",
    "picture_expression = r\"(?:<.{0,2}[Pp]ic.{1,5}>\\s*)(.*?)(?:<.{0,2}[Pp]ic.{1,5}>)\"\n",
    "gmap_id_expression=r\"(?:<.{0,2}[Gg]map.{1,3}>\\s*)(.*?)(?:<.{0,2}[Gg]map.{1,3}>)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = './student_group181'\n",
    "all_text_data = \"\"\n",
    "file_names = [\n",
    "    'group181_0.txt', 'group181_1.txt', 'group181_2.txt', 'group181_3.txt', \n",
    "    'group181_4.txt', 'group181_5.txt', 'group181_6.txt', 'group181_7.txt', \n",
    "    'group181_8.txt', 'group181_9.txt', 'group181_10.txt', 'group181_11.txt',\n",
    "    'group181_12.txt', 'group181_13.txt', 'group181_14.txt'\n",
    "]\n",
    "for file_name in file_names:\n",
    "    file_path = f\"{text_path}/{file_name}\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        all_text_data += content\n",
    "        \n",
    "split_txt = '?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n",
    "final_split= '</dataset><record>|<dataset><record>'\n",
    "folder_text = all_text_data.split(split_txt)\n",
    "if folder_text[0] == \"\":\n",
    "    folder_text.pop(0)\n",
    "final_split_text = []\n",
    "for text in folder_text:\n",
    "    slipted_info = re.split(final_split,text)\n",
    "    final_split_text.extend(slipted_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5q/zdvmc7js3c31nxcnqbmtsfkc0000gn/T/ipykernel_35552/820920072.py:66: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  text_datafram['time'] = pd.to_datetime(text_datafram['time'], unit ='ms', utc = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44697 entries, 0 to 44696\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   user_id      44697 non-null  object \n",
      " 1   name         44697 non-null  object \n",
      " 2   time         44697 non-null  object \n",
      " 3   rating       44697 non-null  float64\n",
      " 4   text         23719 non-null  object \n",
      " 5   pics         867 non-null    object \n",
      " 6   resp         4637 non-null   object \n",
      " 7   gmap_id      44697 non-null  object \n",
      " 8   if_pic       44697 non-null  object \n",
      " 9   if_response  44697 non-null  object \n",
      "dtypes: float64(1), object(9)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "user_id_list = []\n",
    "gmap_id_list = []\n",
    "time_list = []\n",
    "date_list = []\n",
    "rate_list = []\n",
    "review_list = []\n",
    "response_list = []\n",
    "picture_list = []\n",
    "name_list = []\n",
    "\n",
    "user_id_pattern = re.compile(user_id_expression, re.DOTALL)\n",
    "time_pattern = re.compile(time_expression, re.DOTALL)\n",
    "rate_pattern = re.compile(rate_expression, re.DOTALL)\n",
    "rateing_pattern = re.compile(rateing_expression, re.DOTALL)\n",
    "text_pattern = re.compile(text_expression, re.DOTALL)\n",
    "review_pattern = re.compile(review_expression, re.DOTALL)\n",
    "response_pattern = re.compile(response_expression, re.DOTALL)\n",
    "picture_pattern = re.compile(picture_expression, re.DOTALL)\n",
    "name_pattern = re.compile(name_expression,re.DOTALL)\n",
    "gmap_id_pattern = re.compile(gmap_id_expression, re.DOTALL)\n",
    "\n",
    "for text in final_split_text:\n",
    "     \n",
    "     user_id_matches = user_id_pattern.findall(text)\n",
    "     user_id_list.extend(user_id_matches)\n",
    "     \n",
    "     gmap_id_matches = gmap_id_pattern.findall(text)\n",
    "     gmap_id_list.extend(gmap_id_matches)\n",
    "     \n",
    "     time_matches = time_pattern.findall(text)\n",
    "     time_list.extend(time_matches)\n",
    "     \n",
    "     rate_matches = rate_pattern.findall(text)\n",
    "     if not rate_matches:\n",
    "        rate_matches = rateing_pattern.findall(text)\n",
    "     rate_list.extend(rate_matches)\n",
    "     \n",
    "     review_matches = review_pattern.findall(text)\n",
    "     if not review_matches:\n",
    "        review_matches = text_pattern.findall(text)\n",
    "     review_list.extend(review_matches)\n",
    "     \n",
    "     response_matches = response_pattern.findall(text)\n",
    "     response_list.extend(response_matches)\n",
    "     \n",
    "     picture_matches = picture_pattern.findall(text)\n",
    "     picture_list.extend(picture_matches)\n",
    "     \n",
    "     \n",
    "     name_matches = name_pattern.findall(text)\n",
    "     name_list.extend(name_matches)\n",
    "\n",
    "text_datafram = pd.DataFrame({\n",
    "    'user_id': user_id_list,\n",
    "    'name':name_list,\n",
    "    'time': time_list,\n",
    "    'rating': rate_list,\n",
    "    'text': review_list,\n",
    "    'pics': picture_list,\n",
    "    'resp': response_list,\n",
    "    'gmap_id': gmap_id_list\n",
    "})\n",
    "\n",
    "text_datafram = text_datafram[['user_id', 'name', 'time', 'rating', 'text', 'pics', 'resp','gmap_id']]\n",
    "text_datafram['rating'] = text_datafram['rating'].astype(float)\n",
    "text_datafram['time'] = pd.to_datetime(text_datafram['time'], unit ='ms', utc = True)\n",
    "text_datafram['time'] = text_datafram['time'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "text_datafram = text_datafram.replace('None', None)\n",
    "\n",
    "if_pic = []\n",
    "if_response = []\n",
    "for index, row in text_datafram.iterrows():\n",
    "    if row['pics'] is None:\n",
    "        if_pic.append('N')\n",
    "    else:\n",
    "        if_pic.append('Y')\n",
    "    \n",
    "    if row['resp'] is None:\n",
    "        if_response.append('N')\n",
    "    else:\n",
    "        if_response.append('Y')\n",
    "text_datafram['if_pic'] = if_pic\n",
    "text_datafram['if_response'] = if_response\n",
    "\n",
    "text_datafram.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1782 entries, 0 to 1781\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   user_id      1782 non-null   object \n",
      " 1   name         1782 non-null   object \n",
      " 2   time         1782 non-null   object \n",
      " 3   rating       1782 non-null   float64\n",
      " 4   text         1019 non-null   object \n",
      " 5   pics         29 non-null     object \n",
      " 6   resp         151 non-null    object \n",
      " 7   gmap_id      1782 non-null   object \n",
      " 8   if_pic       1782 non-null   object \n",
      " 9   if_response  1782 non-null   object \n",
      "dtypes: float64(1), object(9)\n",
      "memory usage: 139.3+ KB\n"
     ]
    }
   ],
   "source": [
    "xlsx_path = \"./student_group181/group181.xlsx\"\n",
    "xlsx = pd.ExcelFile(xlsx_path)\n",
    "all_sheets = pd.read_excel(xlsx_path, sheet_name=None)\n",
    "cleaned_sheets = []\n",
    "for sheet_name, df in all_sheets.items():\n",
    "    df = df.dropna(how='all')\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    if not df.empty:\n",
    "        cleaned_sheets.append(df)\n",
    "\n",
    "xlsx_data = pd.concat(cleaned_sheets, ignore_index=True)\n",
    "xlsx_data = xlsx_data[['user_id', 'name', 'time', 'rating', 'text', 'pics', 'resp','gmap_id']]\n",
    "xlsx_data['time'] = pd.to_datetime(xlsx_data['time'], unit ='ms', utc = True)\n",
    "xlsx_data['time'] = xlsx_data['time'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "xlsx_data['pics'] = xlsx_data['pics'].where(pd.notna(xlsx_data['pics']), None)\n",
    "xlsx_data['resp'] = xlsx_data['resp'].where(pd.notna(xlsx_data['resp']), None)\n",
    "\n",
    "if_pic_xlsx = []\n",
    "if_response_xlsx = []\n",
    "for index, row in xlsx_data.iterrows():\n",
    "    # 如果 'pics' 列为 None，添加 'N'，否则添加 'Y'\n",
    "    if row['pics'] is None:\n",
    "        if_pic_xlsx.append('N')\n",
    "    else:\n",
    "        if_pic_xlsx.append('Y')\n",
    "    \n",
    "    # 如果 'resp' 列为 None，添加 'N'，否则添加 'Y'\n",
    "    if row['resp'] is None:\n",
    "        if_response_xlsx.append('N')\n",
    "    else:\n",
    "        if_response_xlsx.append('Y')\n",
    "\n",
    "xlsx_data['if_pic'] = if_pic_xlsx\n",
    "xlsx_data['if_response'] = if_response_xlsx\n",
    "\n",
    "xlsx_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTqmBHIKiqK_"
   },
   "source": [
    "These patterns are used in the next step when reading the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZ-njkJciqK_"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGcAMvmhiqK_"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.2. Reading Files <a class=\"anchor\" name=\"Read\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llC5D5M3iqK_"
   },
   "source": [
    "In this step, all files are read and parsed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-brytUfiqLA"
   },
   "source": [
    "Let's take a look at the first ten elements of the lists generated. We can see that ids, reviews,etc. are parsed and stored correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MD-LSS76iqLA"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtwS6ttqiqLA"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.3. Whatever else <a class=\"anchor\" name=\"latin\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IJ63oV9iqLA"
   },
   "source": [
    "the rest of your methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g9k9Fb8iqLB"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "KVwmp1LfiqLE"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 5.  Writing to JSON File <a class=\"anchor\" name=\"write\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFHcHFPGiqLE"
   },
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0WT10TJiqLE"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "3XcaJBATiqLE"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 5.1. Verification of the Generated JSON File <a class=\"anchor\" name=\"test_xml\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO8vwKqkiqLF"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 6. Summary <a class=\"anchor\" name=\"summary\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEg_xQdXiqLF"
   },
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20RDw_JDiqLF"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QdX7ozQiqLF"
   },
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnASfTmniqLF"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li7bchX9iqLF"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 7. References <a class=\"anchor\" name=\"Ref\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkWuWC3NiqLF"
   },
   "source": [
    "\n",
    "\n",
    "[1]<a class=\"anchor\" name=\"ref-2\"></a> Why do I need to add DOTALL to python regular expression to match new line in raw string, https://stackoverflow.com/questions/22610247, Accessed 30/08/2022.\n",
    "\n",
    "....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVyuz4LciqLG"
   },
   "source": [
    "## --------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task1_xxxxxxx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
