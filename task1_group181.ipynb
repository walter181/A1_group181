{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCbAmQ47iqK4"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# FIT5196 Task 1 in Assessment 1\n",
    "#### Student Name: Deshui Yu      Liangjing Yang\n",
    "#### Student ID: 34253599      34060871\n",
    "\n",
    "Date: 21/08/2024\n",
    "\n",
    "\n",
    "Environment: Python xxxx\n",
    "\n",
    "Libraries used:\n",
    "* re (for regular expression, installed and imported) \n",
    "* pandas (for data manipulation) \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjBFqYK4iqK5"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "## Table of Contents\n",
    "\n",
    "</div>    \n",
    "\n",
    "[1. Introduction](#Intro) <br>\n",
    "[2. Importing Libraries](#libs) <br>\n",
    "[3. Examining Patent Files](#examine) <br>\n",
    "[4. Loading and Parsing Files](#load) <br>\n",
    "$\\;\\;\\;\\;$[4.1. Defining Regular Expressions](#Reg_Exp) <br>\n",
    "$\\;\\;\\;\\;$[4.2. Reading Files](#Read) <br>\n",
    "$\\;\\;\\;\\;$[4.3. Whatever else](#latin) <br>\n",
    "[5. Writing to CSV/JSON File](#write) <br>\n",
    "$\\;\\;\\;\\;$[5.1. Verification - using the sample files](#test_xml) <br>\n",
    "[6. Summary](#summary) <br>\n",
    "[7. References](#Ref) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcbqK3KliqK6"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEFdSCIUiqK6"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGg4-8VSiqK6"
   },
   "source": [
    "This assessment regards extracting data from semi-sctuctured text files. The dataset contained 500 `.txt` files which included various information about user reviews. In particular, ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6Ql-W6BiqK7"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnctlBF6iqK7"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQOLlwmAiqK7"
   },
   "source": [
    "The packages to be used in this assessment are imported in the following. They are used to fulfill the following tasks:\n",
    "\n",
    "* **re:** to define and use regular expressions\n",
    "* **pandas:** ...\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mKGO6FAXiqK7"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# My assignment is running locally, and I have already downloaded the data to my local system, so I don't need to link to Google Drive.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DEWD9qIiqK8"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Z814ttFYiqK8"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## 3.  Examining Raw Data <a class=\"anchor\" name=\"examine\"></a>\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YVIqb_miqK8"
   },
   "source": [
    "First of all, open all the files and examine the data within them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gp3TG3fyiqK9"
   },
   "source": [
    "Having examined the file content, the following observations were made:\n",
    "\n",
    "Examining the XLSX File:\n",
    "- By opening the XLSX file, I discovered that there are sheets numbered from 0 to 14. This means I need to combine these sheets into a single DataFrame.\n",
    "- By observing the different sheets, I noticed that some of them contain entire rows and columns of empty values. I think I need to remove these empty values.\n",
    "- After removing the irrelevant empty rows and columns, I found that all sheets have the same content. However, the format of the date and other data does not meet the requirements of the assignment.\n",
    "\n",
    "Examining the TXT Files:\n",
    "- I have 15 files numbered from 0 to 14. The first line of each TXT file contains the label `<?xml version=\"1.0\" encoding=\"UTF-8\"?>`, which is a common point. I can use this to split the data.\n",
    "- I also noticed that the label `</dataset><record>` can be used to split the data for each user, and this pattern is consistent across all TXT files.\n",
    "- In the TXT files, each piece of data is wrapped in tags, such as userid, username, rate, pictures, etc. Additionally, the tags are quite varied; for instance, the rate data can have multiple tag forms such as `<Rate>`, `</rate>`, `<rating>`, `</Rating>`, and more. This means that if I want to extract the data, my regular expressions will need to be compatible with these different tag forms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBEASWLfiqK-"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "iDoVeDSHiqK-"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 4.  Loading and Parsing Files <a class=\"anchor\" name=\"load\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z24HaN8hiqK-"
   },
   "source": [
    "In this section, the files are parsed and processed. First of all, appropriate regular expressions are defined to extract desired information when reading the files. ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ0tuwvZiqK-"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rApp_Ic9iqK-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.1. Defining Regular Expressions <a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knauV0VViqK-"
   },
   "source": [
    "Defining correct regular expressions is crucial in extracting desired information from the text efficiently. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_expression = r\"<[^>]*?ser[^>]*?>\\s*(\\d{21})\\s*<[^>]*?/[^>]*?ser[^>]*?>\"\n",
    "time_expression = r\"(?:<.{1,3}ime\\s*/?>\\s*|<.{0,2}[Dd]ate\\s*/?>\\s*)(\\d+)(?:\\s*<\\s*/\\s*.{1,3}ime\\s*/?>|<\\s*/\\s*.{0,2}[Dd]ate\\s*/?>)\"\n",
    "rate_expression = r\"(?:<.{0,2}[Rr]ate>\\s*)(\\d+)(?:<.{0,2}[Rr]ate>)\"\n",
    "rateing_expression = r\"(?:<.{0,2}[Rr]ating>\\s*)(\\d+)(?:<.{0,2}[Rr]ating>)\"\n",
    "text_expression = r\"(?:<.{0,3}[Tt]ext>\\s*)(.*?)(?:<.{0,3}[Tt]ext>)\"\n",
    "review_expression = r\"(?:<.{0,2}[Rr]eview>\\s*)(.*?)(?:<.{0,2}[Rr]eview>)\"\n",
    "name_expression = r\"(?:<.{0,7}[Nn]ame>\\s*)(.*?)(?:<.{0,7}[Nn]ame\\s*>)\"\n",
    "response_expression = r\"(?:<.{0,3}[Rr]esp.{0,4}>\\s*)(.*?)(?:<.{0,3}[Rr]esp.{0,4}>)\"\n",
    "picture_expression = r\"(?:<.{0,2}[Pp]ic.{1,5}>\\s*)(.*?)(?:<.{0,2}[Pp]ic.{1,5}>)\"\n",
    "gmap_id_expression=r\"(?:<.{0,2}[Gg]map.{1,3}>\\s*)(.*?)(?:<.{0,2}[Gg]map.{1,3}>)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = './student_group181'\n",
    "all_text_data = \"\"\n",
    "file_names = [\n",
    "    'group181_0.txt', 'group181_1.txt', 'group181_2.txt', 'group181_3.txt', \n",
    "    'group181_4.txt', 'group181_5.txt', 'group181_6.txt', 'group181_7.txt', \n",
    "    'group181_8.txt', 'group181_9.txt', 'group181_10.txt', 'group181_11.txt',\n",
    "    'group181_12.txt', 'group181_13.txt', 'group181_14.txt'\n",
    "]\n",
    "for file_name in file_names:\n",
    "    file_path = f\"{text_path}/{file_name}\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        all_text_data += content\n",
    "        \n",
    "split_txt = '?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n",
    "final_split= '</dataset><record>|<dataset><record>'\n",
    "folder_text = all_text_data.split(split_txt)\n",
    "if folder_text[0] == \"\":\n",
    "    folder_text.pop(0)\n",
    "final_split_text = []\n",
    "for text in folder_text:\n",
    "    slipted_info = re.split(final_split,text)\n",
    "    final_split_text.extend(slipted_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>pics</th>\n",
       "      <th>resp</th>\n",
       "      <th>gmap_id</th>\n",
       "      <th>if_pic</th>\n",
       "      <th>if_response</th>\n",
       "      <th>pic_dim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103499789493991350231</td>\n",
       "      <td>Alyson Hoffman</td>\n",
       "      <td>2021-06-01 04:57:49</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Booked last minute rentals here over Memorial ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0x808e6a7f6b666d4f:0xfd9214abe1df3a4d</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>[ ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105404622065259965715</td>\n",
       "      <td>OnlyOneEarth</td>\n",
       "      <td>2021-03-21 07:59:56</td>\n",
       "      <td>5.0</td>\n",
       "      <td>When I went into Epicenter Cycling, I was look...</td>\n",
       "      <td>[{'url': ['https://lh5.googleusercontent.com/p...</td>\n",
       "      <td>{'time': 1616349255531, 'text': \"We're so happ...</td>\n",
       "      <td>0x808e6a7f6b666d4f:0xfd9214abe1df3a4d</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[[150, 150], [150, 150]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118347770680889723845</td>\n",
       "      <td>matthew thomson</td>\n",
       "      <td>2021-04-11 15:44:41</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Amazing selection of Trek bikes! We were told ...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'time': 1622404153725, 'text': \"Thanks for ta...</td>\n",
       "      <td>0x808e6a7f6b666d4f:0xfd9214abe1df3a4d</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[ ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109541606733835908756</td>\n",
       "      <td>Becky B</td>\n",
       "      <td>2021-03-15 15:42:06</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Absolutely flawless experience ordering, sizin...</td>\n",
       "      <td>[{'url': ['https://lh5.googleusercontent.com/p...</td>\n",
       "      <td>{'time': 1616264680468, 'text': 'Thank you so ...</td>\n",
       "      <td>0x808e6a7f6b666d4f:0xfd9214abe1df3a4d</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[[150, 150]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>108792737827746934756</td>\n",
       "      <td>Tiffany Klitzke</td>\n",
       "      <td>2021-02-02 18:00:03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>All of the employees were so friendly and help...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'time': 1614824613439, 'text': \"Thank you so ...</td>\n",
       "      <td>0x808e6a7f6b666d4f:0xfd9214abe1df3a4d</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[ ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44692</th>\n",
       "      <td>112497754617102655494</td>\n",
       "      <td>Bernard Janov</td>\n",
       "      <td>2020-05-21 18:56:48</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0x80dbfe36c3b7a2c7:0x8e10768a0238d2b9</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>[ ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44693</th>\n",
       "      <td>112173222860157032040</td>\n",
       "      <td>Ernest Ikei</td>\n",
       "      <td>2019-06-07 00:44:22</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0x80dbfe36c3b7a2c7:0x8e10768a0238d2b9</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>[ ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44694</th>\n",
       "      <td>105511521957721541111</td>\n",
       "      <td>Randall Rolfe</td>\n",
       "      <td>2020-09-01 23:44:36</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'time': 1613606949222, 'text': 'Thank you, Ra...</td>\n",
       "      <td>0x80dbfe36c3b7a2c7:0x8e10768a0238d2b9</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[ ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44695</th>\n",
       "      <td>104981526115446298575</td>\n",
       "      <td>Donna Nagel</td>\n",
       "      <td>2020-10-15 19:59:21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'time': 1613605845348, 'text': 'Thank you for...</td>\n",
       "      <td>0x80dbfe36c3b7a2c7:0x8e10768a0238d2b9</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[ ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44696</th>\n",
       "      <td>113663712050490880120</td>\n",
       "      <td>Irenio Macagba</td>\n",
       "      <td>2020-09-26 16:02:32</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'time': 1615227790162, 'text': 'Thank you, Ir...</td>\n",
       "      <td>0x80dbfe36c3b7a2c7:0x8e10768a0238d2b9</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[ ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44697 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user_id             name                 time  rating  \\\n",
       "0      103499789493991350231   Alyson Hoffman  2021-06-01 04:57:49     5.0   \n",
       "1      105404622065259965715     OnlyOneEarth  2021-03-21 07:59:56     5.0   \n",
       "2      118347770680889723845  matthew thomson  2021-04-11 15:44:41     5.0   \n",
       "3      109541606733835908756          Becky B  2021-03-15 15:42:06     5.0   \n",
       "4      108792737827746934756  Tiffany Klitzke  2021-02-02 18:00:03     5.0   \n",
       "...                      ...              ...                  ...     ...   \n",
       "44692  112497754617102655494    Bernard Janov  2020-05-21 18:56:48     5.0   \n",
       "44693  112173222860157032040      Ernest Ikei  2019-06-07 00:44:22     5.0   \n",
       "44694  105511521957721541111    Randall Rolfe  2020-09-01 23:44:36     5.0   \n",
       "44695  104981526115446298575      Donna Nagel  2020-10-15 19:59:21     5.0   \n",
       "44696  113663712050490880120   Irenio Macagba  2020-09-26 16:02:32     5.0   \n",
       "\n",
       "                                                    text  \\\n",
       "0      Booked last minute rentals here over Memorial ...   \n",
       "1      When I went into Epicenter Cycling, I was look...   \n",
       "2      Amazing selection of Trek bikes! We were told ...   \n",
       "3      Absolutely flawless experience ordering, sizin...   \n",
       "4      All of the employees were so friendly and help...   \n",
       "...                                                  ...   \n",
       "44692                                               None   \n",
       "44693                                               None   \n",
       "44694                                               None   \n",
       "44695                                               None   \n",
       "44696                                               None   \n",
       "\n",
       "                                                    pics  \\\n",
       "0                                                   None   \n",
       "1      [{'url': ['https://lh5.googleusercontent.com/p...   \n",
       "2                                                   None   \n",
       "3      [{'url': ['https://lh5.googleusercontent.com/p...   \n",
       "4                                                   None   \n",
       "...                                                  ...   \n",
       "44692                                               None   \n",
       "44693                                               None   \n",
       "44694                                               None   \n",
       "44695                                               None   \n",
       "44696                                               None   \n",
       "\n",
       "                                                    resp  \\\n",
       "0                                                   None   \n",
       "1      {'time': 1616349255531, 'text': \"We're so happ...   \n",
       "2      {'time': 1622404153725, 'text': \"Thanks for ta...   \n",
       "3      {'time': 1616264680468, 'text': 'Thank you so ...   \n",
       "4      {'time': 1614824613439, 'text': \"Thank you so ...   \n",
       "...                                                  ...   \n",
       "44692                                               None   \n",
       "44693                                               None   \n",
       "44694  {'time': 1613606949222, 'text': 'Thank you, Ra...   \n",
       "44695  {'time': 1613605845348, 'text': 'Thank you for...   \n",
       "44696  {'time': 1615227790162, 'text': 'Thank you, Ir...   \n",
       "\n",
       "                                     gmap_id if_pic if_response  \\\n",
       "0      0x808e6a7f6b666d4f:0xfd9214abe1df3a4d      N           N   \n",
       "1      0x808e6a7f6b666d4f:0xfd9214abe1df3a4d      Y           Y   \n",
       "2      0x808e6a7f6b666d4f:0xfd9214abe1df3a4d      N           Y   \n",
       "3      0x808e6a7f6b666d4f:0xfd9214abe1df3a4d      Y           Y   \n",
       "4      0x808e6a7f6b666d4f:0xfd9214abe1df3a4d      N           Y   \n",
       "...                                      ...    ...         ...   \n",
       "44692  0x80dbfe36c3b7a2c7:0x8e10768a0238d2b9      N           N   \n",
       "44693  0x80dbfe36c3b7a2c7:0x8e10768a0238d2b9      N           N   \n",
       "44694  0x80dbfe36c3b7a2c7:0x8e10768a0238d2b9      N           Y   \n",
       "44695  0x80dbfe36c3b7a2c7:0x8e10768a0238d2b9      N           Y   \n",
       "44696  0x80dbfe36c3b7a2c7:0x8e10768a0238d2b9      N           Y   \n",
       "\n",
       "                        pic_dim  \n",
       "0                           [ ]  \n",
       "1      [[150, 150], [150, 150]]  \n",
       "2                           [ ]  \n",
       "3                  [[150, 150]]  \n",
       "4                           [ ]  \n",
       "...                         ...  \n",
       "44692                       [ ]  \n",
       "44693                       [ ]  \n",
       "44694                       [ ]  \n",
       "44695                       [ ]  \n",
       "44696                       [ ]  \n",
       "\n",
       "[44697 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id_list = []\n",
    "gmap_id_list = []\n",
    "time_list = []\n",
    "date_list = []\n",
    "rate_list = []\n",
    "review_list = []\n",
    "response_list = []\n",
    "picture_list = []\n",
    "name_list = []\n",
    "\n",
    "user_id_pattern = re.compile(user_id_expression, re.DOTALL)\n",
    "time_pattern = re.compile(time_expression, re.DOTALL)\n",
    "rate_pattern = re.compile(rate_expression, re.DOTALL)\n",
    "rateing_pattern = re.compile(rateing_expression, re.DOTALL)\n",
    "text_pattern = re.compile(text_expression, re.DOTALL)\n",
    "review_pattern = re.compile(review_expression, re.DOTALL)\n",
    "response_pattern = re.compile(response_expression, re.DOTALL)\n",
    "picture_pattern = re.compile(picture_expression, re.DOTALL)\n",
    "name_pattern = re.compile(name_expression,re.DOTALL)\n",
    "gmap_id_pattern = re.compile(gmap_id_expression, re.DOTALL)\n",
    "\n",
    "for text in final_split_text:\n",
    "     \n",
    "     user_id_matches = user_id_pattern.findall(text)\n",
    "     user_id_list.extend(user_id_matches)\n",
    "     \n",
    "     gmap_id_matches = gmap_id_pattern.findall(text)\n",
    "     gmap_id_list.extend(gmap_id_matches)\n",
    "     \n",
    "     time_matches = time_pattern.findall(text)\n",
    "     time_list.extend(time_matches)\n",
    "     \n",
    "     rate_matches = rate_pattern.findall(text)\n",
    "     if not rate_matches:\n",
    "        rate_matches = rateing_pattern.findall(text)\n",
    "     rate_list.extend(rate_matches)\n",
    "     \n",
    "     review_matches = review_pattern.findall(text)\n",
    "     if not review_matches:\n",
    "        review_matches = text_pattern.findall(text)\n",
    "     review_list.extend(review_matches)\n",
    "     \n",
    "     response_matches = response_pattern.findall(text)\n",
    "     response_list.extend(response_matches)\n",
    "     \n",
    "     picture_matches = picture_pattern.findall(text)\n",
    "     picture_list.extend(picture_matches)\n",
    "     \n",
    "     \n",
    "     name_matches = name_pattern.findall(text)\n",
    "     name_list.extend(name_matches)\n",
    "\n",
    "text_datafram = pd.DataFrame({\n",
    "    'user_id': user_id_list,\n",
    "    'name':name_list,\n",
    "    'time': time_list,\n",
    "    'rating': rate_list,\n",
    "    'text': review_list,\n",
    "    'pics': picture_list,\n",
    "    'resp': response_list,\n",
    "    'gmap_id': gmap_id_list\n",
    "})\n",
    "\n",
    "text_datafram = text_datafram[['user_id', 'name', 'time', 'rating', 'text', 'pics', 'resp','gmap_id']]\n",
    "text_datafram['rating'] = text_datafram['rating'].astype(float)\n",
    "text_datafram['time'] = pd.to_datetime(pd.to_numeric(text_datafram['time']), unit='ms', utc=True)\n",
    "text_datafram['time'] = text_datafram['time'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "text_datafram = text_datafram.replace('None', None)\n",
    "\n",
    "if_pic = []\n",
    "if_response = []\n",
    "for index, row in text_datafram.iterrows():\n",
    "    if row['pics'] is None:\n",
    "        if_pic.append('N')\n",
    "    else:\n",
    "        if_pic.append('Y')\n",
    "    \n",
    "    if row['resp'] is None:\n",
    "        if_response.append('N')\n",
    "    else:\n",
    "        if_response.append('Y')\n",
    "text_datafram['if_pic'] = if_pic\n",
    "text_datafram['if_response'] = if_response\n",
    "def extract_dimensions(text):\n",
    "    if pd.isna(text):\n",
    "        return \"[ ]\"\n",
    "    dimensions = re.findall(r'w(\\d{1,3})-h(\\d{1,3})-k-no-p', text)\n",
    "    if not dimensions:\n",
    "        return \"[ ]\"\n",
    "    return [[int(width), int(height)] for width, height in dimensions]\n",
    "\n",
    "text_datafram['pic_dim'] = text_datafram['pics'].apply(extract_dimensions)\n",
    "text_datafram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "xls_path = \"./student_group181/group181.xlsx\"\n",
    "\n",
    "xls = pd.ExcelFile(xls_path)\n",
    "all_sheets = pd.read_excel(xls_path, sheet_name=None)\n",
    "cleaned_sheets = []\n",
    "for sheet_name, df in all_sheets.items():\n",
    "    df = df.dropna(how='all')\n",
    "    # 去除空的列\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    # 仅在DataFrame有数据时，添加到清理后的列表中\n",
    "    if not df.empty:\n",
    "        cleaned_sheets.append(df)\n",
    "\n",
    "# 将所有清理后的DataFrame合并为一个\n",
    "xls_data = pd.concat(cleaned_sheets, ignore_index=True)\n",
    "xls_data = xls_data[['user_id', 'name', 'time', 'rating', 'text', 'pics', 'resp','gmap_id']]\n",
    "#转化时间格式\n",
    "xls_data['time'] = pd.to_datetime(xls_data['time'], unit ='ms', utc = True)\n",
    "#将time列格式化为字符串格式\n",
    "xls_data['time'] = xls_data['time'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "xls_data['pics'] = xls_data['pics'].where(pd.notna(xls_data['pics']), None)\n",
    "xls_data['resp'] = xls_data['resp'].where(pd.notna(xls_data['resp']), None)\n",
    "\n",
    "if_pic_xls = []\n",
    "if_response_xls = []\n",
    "for index, row in xls_data.iterrows():\n",
    "    # 如果 'pics' 列为 None，添加 'N'，否则添加 'Y'\n",
    "    if row['pics'] is None:\n",
    "        if_pic_xls.append('N')\n",
    "    else:\n",
    "        if_pic_xls.append('Y')\n",
    "    \n",
    "    # 如果 'resp' 列为 None，添加 'N'，否则添加 'Y'\n",
    "    if row['resp'] is None:\n",
    "        if_response_xls.append('N')\n",
    "    else:\n",
    "        if_response_xls.append('Y')\n",
    "\n",
    "\n",
    "\n",
    "xls_data['if_pic'] = if_pic_xls\n",
    "xls_data['if_response'] = if_response_xls\n",
    "\n",
    "def extract_dimensions(text):\n",
    "    if pd.isna(text):\n",
    "        return \"[ ]\"\n",
    "    dimensions = re.findall(r'w(\\d{1,3})-h(\\d{1,3})-k-no-p', text)\n",
    "    if not dimensions:\n",
    "        return \"[ ]\"\n",
    "    return [[int(width), int(height)] for width, height in dimensions]\n",
    "\n",
    "xls_data['pic_dim'] = xls_data['pics'].apply(extract_dimensions)\n",
    "\n",
    "#xls_data\n",
    "\n",
    "combined_data = pd.concat([xls_data, text_datafram], axis=0)\n",
    "#print(combined_data.info())\n",
    "#combined_data = combined_data.apply(lambda x: x.str.encode('utf-8').str.decode('utf-8') if x.dtype == 'object' else x)\n",
    "combined_data['text'] = combined_data['text'].apply(\n",
    "    lambda x: re.sub(\n",
    "        r\"[\\U0001F600-\\U0001F64F\"  \n",
    "        r\"\\U0001F300-\\U0001F5FF\"  \n",
    "        r\"\\U0001F680-\\U0001F6FF\"  \n",
    "        r\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        r\"\\U00002702-\\U000027B0\"  \n",
    "        r\"\\U000024C2-\\U0001F251\"  \n",
    "        r\"]+\", '', x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "combined_data['text'] = combined_data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grouped = combined_data.groupby('gmap_id').agg(\n",
    "    review_count=('gmap_id', 'size'),\n",
    "    review_text_count=('text', lambda x: x.notna().sum()),\n",
    "    response_count=('resp', lambda x: x.notna().sum())\n",
    ").reset_index()\n",
    "\n",
    "grouped.to_csv('task1_group181.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTqmBHIKiqK_"
   },
   "source": [
    "These patterns are used in the next step when reading the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZ-njkJciqK_"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGcAMvmhiqK_"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.2. Reading Files <a class=\"anchor\" name=\"Read\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llC5D5M3iqK_"
   },
   "source": [
    "In this step, all files are read and parsed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-brytUfiqLA"
   },
   "source": [
    "Let's take a look at the first ten elements of the lists generated. We can see that ids, reviews,etc. are parsed and stored correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MD-LSS76iqLA"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtwS6ttqiqLA"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.3. Whatever else <a class=\"anchor\" name=\"latin\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IJ63oV9iqLA"
   },
   "source": [
    "the rest of your methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g9k9Fb8iqLB"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "KVwmp1LfiqLE"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 5.  Writing to JSON File <a class=\"anchor\" name=\"write\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFHcHFPGiqLE"
   },
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0WT10TJiqLE"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "3XcaJBATiqLE"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 5.1. Verification of the Generated JSON File <a class=\"anchor\" name=\"test_xml\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO8vwKqkiqLF"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 6. Summary <a class=\"anchor\" name=\"summary\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEg_xQdXiqLF"
   },
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20RDw_JDiqLF"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QdX7ozQiqLF"
   },
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnASfTmniqLF"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li7bchX9iqLF"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 7. References <a class=\"anchor\" name=\"Ref\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkWuWC3NiqLF"
   },
   "source": [
    "\n",
    "\n",
    "[1]<a class=\"anchor\" name=\"ref-2\"></a> Why do I need to add DOTALL to python regular expression to match new line in raw string, https://stackoverflow.com/questions/22610247, Accessed 30/08/2022.\n",
    "\n",
    "....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVyuz4LciqLG"
   },
   "source": [
    "## --------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task1_xxxxxxx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
