{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8tinZOUlDER"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "# FIT5196 Task 2 in Assessment 1\n",
    "    \n",
    "#### Student Name: Deshui Yu\n",
    "#### Student ID: 34253599\n",
    "\n",
    "Date: 24/08/2024\n",
    "\n",
    "Environment: xxxxxx\n",
    "\n",
    "Libraries used:\n",
    "* os (for interacting with the operating system, included in Python xxxx) \n",
    "* pandas 1.1.0 (for dataframe, installed and imported) \n",
    "* multiprocessing (for performing processes on multi cores, included in Python 3.6.9 package) \n",
    "* itertools (for performing operations on iterables)\n",
    "* nltk 3.5 (Natural Language Toolkit, installed and imported)\n",
    "* nltk.tokenize (for tokenization, installed and imported)\n",
    "* nltk.stem (for stemming the tokens, installed and imported)\n",
    "\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnnLnFnLlDEU"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Table of Contents\n",
    "\n",
    "</div>\n",
    "\n",
    "[1. Introduction](#Intro) <br>\n",
    "[2. Importing Libraries](#libs) <br>\n",
    "[3. Examining Input File](#examine) <br>\n",
    "[4. Loading and Parsing Files](#load) <br>\n",
    "$\\;\\;\\;\\;$[4.1. Tokenization](#tokenize) <br>\n",
    "$\\;\\;\\;\\;$[4.2. Whatever else](#whetev) <br>\n",
    "$\\;\\;\\;\\;$[4.3. Genegrate numerical representation](#whetev1) <br>\n",
    "[5. Writing Output Files](#write) <br>\n",
    "$\\;\\;\\;\\;$[5.1. Vocabulary List](#write-vocab) <br>\n",
    "$\\;\\;\\;\\;$[5.2. Sparse Matrix](#write-sparseMat) <br>\n",
    "[6. Summary](#summary) <br>\n",
    "[7. References](#Ref) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8mo6PPRlDEU"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewZrff73lDEV"
   },
   "source": [
    "This assessment concerns textual data and the aim is to extract data, process them, and transform them into a proper format. The dataset provided is in the format of a PDF file containing ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSr_kwKclDEV"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acwZw2NklDEW"
   },
   "source": [
    "In this assessment, any python packages is permitted to be used. The following packages were used to accomplish the related tasks:\n",
    "\n",
    "* **os:** to interact with the operating system, e.g. navigate through folders to read files\n",
    "* **re:** to define and use regular expressions\n",
    "* **pandas:** to work with dataframes\n",
    "* **multiprocessing:** to perform processes on multi cores for fast performance \n",
    "* **langid:** to detect the language of the text data, ensuring it is in English.\n",
    "* **itertools.chain:** to flatten lists or iterate over multiple lists consecutively.\n",
    "* **nltk (Natural Language Toolkit):** a comprehensive library for natural language processing tasks.\n",
    "* **sklearn.feature_extraction.text.CountVectorizer:** to convert a collection of text documents into a matrix of token counts.\n",
    "* **collections.defaultdict:** to create dictionaries with default values for missing keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qgmGWs8HlDEW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import langid\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.probability import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "import json\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwNp0KnWlDEX"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "SA7fSJiRlDEY"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 3.  Examining Input File <a class=\"anchor\" name=\"examine\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bPCuEl8smTHW"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CJDLDI6lDEY"
   },
   "source": [
    "Let's examine what is the content of the file. For this purpose, i need open the json file.In the previous assignment, it was required to convert all review comments in the JSON file to lowercase and remove any emojis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpDVyW4YlDEZ"
   },
   "source": [
    "It is noticed that the file contains several reviews associated with various businesses. These reviews are in the form of JSON objects, where each review includes fields such as review_text, review_rating, user_id, and others. The goal is to preprocess this textual data by performing specific operations like lowercasing, removing emojis, tokenization, and filtering, to ultimately create a vocabulary list and a sparse numerical representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7c1-c3olDEa"
   },
   "source": [
    "Having parsed the pdf file, the following observations can be made:\n",
    "\n",
    "Review Text Characteristics:\n",
    "The review_text field must be normalized to lowercase and stripped of emojis.\n",
    "Tokenization should handle punctuation, special characters, and numeric values appropriately.\n",
    "\n",
    "Business Review Count:\n",
    "Only businesses with at least 70 text reviews will be processed to ensure robust vocabulary generation.\n",
    "\n",
    "Preprocessing Requirements:\n",
    "Validate that the text is in English, convert it to lowercase, and remove emojis.\n",
    "Perform tokenization, stemming, stopword removal, and filter out rare and short tokens.\n",
    "\n",
    "Vocabulary and Count Vector Generation:\n",
    "Generate both unigrams and meaningful bigrams, selecting the top 200 bigrams based on the PMI measure.\n",
    "Output a sorted vocabulary list and a sparse numerical representation mapping business IDs to token frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "ENnHWjoXlDEc"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 4.  Loading and Parsing File <a class=\"anchor\" name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9esGMx8lDEc"
   },
   "source": [
    "In this section, I read the business review-related data from task1_181.json and initialized an empty dictionary called Temp_review_dictionary to store qualifying review_text entries. For each valid dictionary entry, the code retrieves the list of reviews associated with the business. It then extracts the review_text from each review, ensuring the text is not None or an empty string, and stores it in a temporary list for that business. After processing all reviews for the business, the code checks if the number of reviews meets or exceeds 70. If this threshold is met, the reviews are added to Temp_review_dictionary under the corresponding business key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and load the JSON file\n",
    "with open('task1_181.json', 'r', encoding='utf-8') as file:\n",
    "    all_data = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to store qualifying review_text\n",
    "Temp_review_dictionary = {}\n",
    "\n",
    "# Iterate over all items in the data\n",
    "for key, value in all_data.items():\n",
    "    if isinstance(value, dict):\n",
    "        reviews = value.get(\"reviews\", [])\n",
    "        if isinstance(reviews, list):\n",
    "            business_reviews = []  # To store all review_text for the current business\n",
    "            for review in reviews:\n",
    "                # Get the review_text\n",
    "                row_review_text = review.get(\"review_text\")\n",
    "                if row_review_text:  # Ensure review_text is not None or an empty string\n",
    "                    row_review_text = str(row_review_text)\n",
    "                    business_reviews.append(row_review_text)\n",
    "            # If the number of reviews for the current business is 70 or more, save the reviews\n",
    "            if len(business_reviews) >= 70:\n",
    "                Temp_review_dictionary[key] = business_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uh9oUXAlDEd"
   },
   "source": [
    "Let's examine the dictionary generated. For counting the total number of reviews extracted ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reviews = sum(len(reviews) for reviews in Temp_review_dictionary.values())\n",
    "print(total_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "VIfQCD1VlDEe"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 4.1. Tokenization and Generate numerical representatio<a class=\"anchor\" name=\"tokenize\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gchByyjolDEf"
   },
   "source": [
    "Tokenization is a principal step in text processing and producing unigrams. In this section, I first tokenize the review text into words using a regular expression, then store the tokenized reviews back into the dictionary. Next, it flattens the tokenized word lists and removes both context-independent and context-dependent stopwords. The code then stems the words using the Porter stemmer and removes rare words that appear in less than 5% of the reviews. Finally, it filters out any tokens that are shorter than 3 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8zT4N0RlDEf"
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\")\n",
    "# Iterate over all business reviews and perform tokenizati\n",
    "for business_id, reviews in Temp_review_dictionary.items():\n",
    "    tokenized_reviews = []  # To store tokenized reviews for each business\n",
    "    for review in reviews:\n",
    "        # Tokenize\n",
    "        tokens = tokenizer.tokenize(review)\n",
    "        tokenized_reviews.append(tokens)\n",
    "    \n",
    "    # Update the review_dictionary with tokenized reviews\n",
    "    Temp_review_dictionary[business_id] = tokenized_reviews\n",
    "review_dictionary = {}\n",
    "\n",
    "# Flatten the list of tokenized words for each business\n",
    "for review_id, word_list in Temp_review_dictionary.items():\n",
    "    flattened_list = list(chain.from_iterable(word_list))\n",
    "    review_dictionary[review_id] = flattened_list\n",
    "# review_dictionary\n",
    "\n",
    "# Load the context-independent stopwords from a file    \n",
    "with open('stopwords_en.txt', 'r') as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "# Remove context-independent stopwords from the dictionary\n",
    "for id, flattened_tokens in review_dictionary.items():\n",
    "    filtered_tokens = []\n",
    "    for word in flattened_tokens:\n",
    "        if word not in stopwords:\n",
    "            filtered_tokens.append(word)# Remove context-independent stopwords from the dictionary\n",
    "    review_dictionary[id] = filtered_tokens\n",
    "\n",
    "# Calculate document frequency for each word\n",
    "doc_freq = FreqDist()\n",
    "for wordList in review_dictionary.values():\n",
    "    unique_words = set(wordList)\n",
    "    for word in unique_words:\n",
    "        doc_freq[word] += 1\n",
    "# Identify context-dependent stopwords that appear in more than 95% of the businesses\n",
    "threshold = 0.95 * len(review_dictionary)\n",
    "context_dependent_stopwords = [word for word, count in doc_freq.items() if count > threshold]\n",
    "print(context_dependent_stopwords)\n",
    "# Remove context-dependent stopwords from the dictionary\n",
    "for id, flattened_tokens in review_dictionary.items():\n",
    "    filtered_tokens = []\n",
    "    for word in flattened_tokens:\n",
    "        if word not in context_dependent_stopwords:\n",
    "            filtered_tokens.append(word)\n",
    "\n",
    "    review_dictionary[id] = filtered_tokens\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# Stem each word in the dictionary\n",
    "for id, flattened_tokens in review_dictionary.items():\n",
    "    stemmed_tokens = []\n",
    "    for word in flattened_tokens:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        stemmed_tokens.append(stemmed_word)\n",
    "    # Update the dictionary with stemmed tokens\n",
    "    review_dictionary[id] = stemmed_tokens\n",
    "# Recalculate document frequency after stemming\n",
    "doc_freq = FreqDist()\n",
    "for wordList in review_dictionary.values():\n",
    "    unique_words = set(wordList)\n",
    "    for word in unique_words:\n",
    "        doc_freq[word] += 1\n",
    "# Identify rare words that appear in less than 5% of the businesses\n",
    "threshold = 0.05 * len(review_dictionary)\n",
    "rare_words = [word for word, count in doc_freq.items() if count < threshold]\n",
    "print(rare_words)\n",
    "# Remove rare words from the dictionary\n",
    "for id, flattened_tokens in review_dictionary.items():\n",
    "    filtered_tokens = []\n",
    "    for word in flattened_tokens:\n",
    "        if word not in rare_words:\n",
    "            filtered_tokens.append(word)\n",
    "    review_dictionary[id] = filtered_tokens\n",
    "\n",
    "filtered_review_dictionary = {}\n",
    "# Remove tokens with a length of fewer than 3 characters\n",
    "for id, flattened_tokens in review_dictionary.items():\n",
    "    filtered_tokens = [word for word in flattened_tokens if len(word) >= 3]\n",
    "    filtered_review_dictionary[id] = filtered_tokens\n",
    "# Final processed dictionary\n",
    "review_dictionary = filtered_review_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqZos1q6lDEf"
   },
   "source": [
    "The above operation results in a dictionary with PID representing keys and a single string for all reviews of the day concatenated to each other. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_review_dictionary = {}\n",
    "for pid, tokens in review_dictionary.items():\n",
    "    concatenated_review_dictionary[pid] = ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVqFfwwMlDEg"
   },
   "source": [
    "At this stage, all reviews for each PID are tokenized and are stored as a value in the new dictionary (separetely for each day).\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmaGJYIJlDEl"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 5. Writing Output Files <a class=\"anchor\" name=\"write\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjMBqRetlDEl"
   },
   "source": [
    "files need to be generated:\n",
    "* Vocabulary list\n",
    "* Sparse matrix (count_vectors)\n",
    "This is performed in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc6tQ4ljlDEm"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 5.1. Vocabulary List <a class=\"anchor\" name=\"write-vocab\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDlbpGYilDEm"
   },
   "source": [
    "List of vocabulary should also be written to a file, sorted alphabetically, with their reference codes in front of them. This file also refers to the sparse matrix in the next file. For this purpose, This code identifies the top 200 bigrams using Pointwise Mutual Information (PMI) and combines them with unigrams from the tokenized reviews to create a vocabulary. The vocabulary is then sorted alphabetically, each word is assigned an index, and the result is saved to 181_vocab.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6OUXHlxlDEm"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up bigram measures and find top 200 bigrams using PMI\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# Convert the dictionary values (tokenized reviews) to a list for processing\n",
    "tokenized_reviews = list(review_dictionary.values())\n",
    "# Create a BigramCollocationFinder to find the most frequent bigrams in the tokenized reviews\n",
    "finder = BigramCollocationFinder.from_documents(tokenized_reviews)\n",
    "top_200_bigrams = finder.nbest(bigram_measures.pmi, 200)\n",
    "#reference from chatGPT\n",
    "collocated_bigrams = [' '.join(bigram) for bigram in top_200_bigrams]\n",
    "# Create vocabulary from unigrams and top bigrams, then save to a file\n",
    "unigrams = set(chain.from_iterable(review_dictionary.values()))\n",
    "# Combine unigrams and collocated bigrams, then sort them to create the final vocabulary list\n",
    "vocab = sorted(list(unigrams) + collocated_bigrams)\n",
    "with open(\"181_vocab.txt\", \"w\") as vocab_file:\n",
    "    for index, word in enumerate(vocab):\n",
    "        vocab_file.write(f\"{word}:{index}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkGH81YFlDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 5.2. Sparse Matrix <a class=\"anchor\" name=\"write-sparseMat\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtxqUAwmlDEn"
   },
   "source": [
    "For writing sparse matrix for a paper, we firstly calculate the frequency of words for that paper ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__n1fdIqlDEn"
   },
   "outputs": [],
   "source": [
    "# Initialize a CountVectorizer using the previously created vocabulary\n",
    "vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "# Combine tokens for each review into a single string to prepare for vectorization\n",
    "reviews_as_text = [' '.join(tokens) for tokens in review_dictionary.values()]\n",
    "X = vectorizer.transform(reviews_as_text)\n",
    "\n",
    "# 获取每个业务 ID 的顺序列表，以确保 ID 与评论文本一一对应\n",
    "business_ids = list(review_dictionary.keys())\n",
    "# Open a file to write the sparse matrix data\n",
    "with open(f\"181_countvec.txt\", \"w\") as countvec_file:\n",
    "    for i, business_id in enumerate(business_ids):\n",
    "        row = X[i].tocoo()  # Convert the sparse matrix row to COOrdinate format\n",
    "        # Create a list of token indices and their frequencies in the format \"index:frequency\"\n",
    "        tokens_freq = [f\"{col}:{val}\" for col, val in zip(row.col, row.data)]\n",
    "        # Create a line with the business ID followed by the token frequencies, separated by commas\n",
    "        line = f\"{business_id}, \" + \", \".join(tokens_freq) + \"\\n\"\n",
    "        countvec_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUFQU-QXlDEn"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWjri6x_lDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 6. Summary <a class=\"anchor\" name=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXAprlSblDEn"
   },
   "source": [
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes business reviews by loading data from a JSON file, filtering reviews for businesses with at least 70 reviews, and performing text preprocessing steps such as tokenization, stopword removal, stemming, and filtering of rare and short words. It then identifies the top 200 bigrams using PMI, combines them with unigrams to create a vocabulary, and saves this vocabulary to a file. Finally, the script vectorizes the reviews using CountVectorizer and writes the sparse matrix representation of the review data, mapped to business IDs, to an output file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFXYKxO8lDEn"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HppxDtWNlDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 7. References <a class=\"anchor\" name=\"Ref\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCkWr-M1lDEo"
   },
   "source": [
    "[1] Pandas dataframe.drop_duplicates(), https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/, Accessed 13/08/2022.\n",
    "We use ChatGPT to help us improve text expression, correct grammar, provide assignment step suggestions, offer code advice, debug code, and check and enhance regular expressions.https://chatgpt.com/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp9O-a1UlDEo"
   },
   "source": [
    "## --------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task2_xxxxxxx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
